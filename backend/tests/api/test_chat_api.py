import pytest
from pathlib import Path
from uuid import UUID # Import UUID for type checking
from datetime import datetime # Import datetime for type checking

from httpx import AsyncClient

# Import the main app instance to override dependencies
from backend.app.main import app
# Import the dependency provider functions we want to override
from backend.app.api.chat import get_ollama_generate, get_ollama_stream
# Import settings to allow monkeypatching
from backend.app.core.config import Settings, get_settings
# Import our mock factories
from backend.tests.mocks.mock_llm import (
    create_mock_llm_generate_func,
    create_mock_llm_stream_func,
    DEFAULT_NOT_FOUND_RESPONSE
)

# Define paths relative to this test file
TESTS_DIR = Path(__file__).parent
FIXTURES_DIR = TESTS_DIR.parent / "fixtures"
QA_FILE_PATH = FIXTURES_DIR / "mock_qa_pairs.json"
# Define the output directory for JSON generated by these API tests
API_TEST_SAVE_DIR_RELATIVE = Path("backend/tests/fixtures/test_json_sessions/api_tests")
API_TEST_SAVE_DIR_ABSOLUTE = (TESTS_DIR.parent.parent / API_TEST_SAVE_DIR_RELATIVE).resolve()

# --- Test Setup: Dependency Overrides --- #

# Create instances of our MOCK LLM functions
mock_generate = create_mock_llm_generate_func(QA_FILE_PATH)
mock_stream = create_mock_llm_stream_func(QA_FILE_PATH)

# Define functions that return our mocks (to be used in overrides)
def override_get_ollama_generate():
    return mock_generate

def override_get_ollama_stream():
    return mock_stream

# Function to create settings object with modified path for testing
def get_test_settings() -> Settings:
    test_settings = Settings() # Create a new instance
    test_settings.CHAT_RESPONSE_SAVE_DIR = API_TEST_SAVE_DIR_RELATIVE # Override the path
    return test_settings

# Fixture to manage dependency overrides for the whole module
@pytest.fixture(scope="module", autouse=True)
def setup_api_test_overrides():
    """Applies and cleans up dependency overrides for API tests."""
    # Apply overrides before any tests run
    original_overrides = app.dependency_overrides.copy()
    app.dependency_overrides[get_ollama_generate] = override_get_ollama_generate
    app.dependency_overrides[get_ollama_stream] = override_get_ollama_stream
    app.dependency_overrides[get_settings] = get_test_settings # Override settings

    # Ensure the test output directory exists
    API_TEST_SAVE_DIR_ABSOLUTE.mkdir(parents=True, exist_ok=True)
    print(f"\n[API Test Setup] Overrode get_settings. Save dir: {API_TEST_SAVE_DIR_RELATIVE}")
    print(f"[API Test Setup] Ensured directory exists: {API_TEST_SAVE_DIR_ABSOLUTE}")

    yield # Let tests run

    # Cleanup: Restore original overrides
    app.dependency_overrides = original_overrides
    print("\n[API Test Cleanup] Restored dependency overrides.")
    # Optional: Clean up generated files
    # import shutil
    # if API_TEST_SAVE_DIR_ABSOLUTE.exists():
    #     shutil.rmtree(API_TEST_SAVE_DIR_ABSOLUTE)
    #     print(f"[API Test Cleanup] Removed directory: {API_TEST_SAVE_DIR_ABSOLUTE}")

# --- Test Cases --- #

@pytest.mark.asyncio
async def test_chat_endpoint_success(client: AsyncClient):
    """Test successful non-streaming chat request using dependency override."""
    request_data = {
        "prompt": "Hello", 
        "session_id": "api_test_gen_dep",
        "model_name": "mock-model-request"
    }
    expected_response_text = "Mock Hi there! This is a predefined answer."
    expected_model_in_response = "mock-qa-gen-v1" 

    response = await client.post("/api/v1/chat/", json=request_data)

    assert response.status_code == 200
    response_json = response.json()

    assert response_json["request"]["prompt"] == request_data["prompt"]
    assert response_json["request"]["session_id"] == request_data["session_id"]
    assert response_json["request"]["model_name"] == request_data["model_name"]

    assert response_json["response"] == expected_response_text
    assert response_json["model_name"] == expected_model_in_response
    assert response_json["finish_reason"] == "stop"

    assert "response_id" in response_json
    assert isinstance(UUID(response_json["response_id"]), UUID)

    assert "created_at" in response_json
    assert isinstance(datetime.fromisoformat(response_json["created_at"]), datetime)

    assert "completed_at" in response_json
    assert isinstance(datetime.fromisoformat(response_json["completed_at"]), datetime)

    assert "elapsed_time_ms" in response_json
    assert isinstance(response_json["elapsed_time_ms"], float)
    assert response_json["elapsed_time_ms"] > 0

@pytest.mark.asyncio
async def test_chat_stream_endpoint_success(client: AsyncClient):
    """Test successful streaming chat request using dependency override."""
    expected_full_response = "Why don't scientists trust atoms? Because they make up everything! (Mock Joke)"
    request_data = {
        "prompt": "Tell me a joke", 
        "session_id": "api_test_stream_dep",
        "model_name": "mock-stream-request"
    }

    response = await client.post("/api/v1/chat/stream", json=request_data)

    assert response.status_code == 200
    assert response.headers["content-type"] == "text/plain; charset=utf-8"

    streamed_text = await response.aread()
    assert streamed_text.decode() == expected_full_response

@pytest.mark.asyncio
async def test_chat_endpoint_not_found(client: AsyncClient):
    """Test non-streaming endpoint with a prompt not in mock data."""
    request_data = {
        "prompt": "Does not exist", 
        "session_id": "api_test_gen_nf",
        "model_name": "mock-model-request-nf"
    }
    expected_model_in_response = "mock-qa-gen-v1"

    response = await client.post("/api/v1/chat/", json=request_data)
    assert response.status_code == 200
    response_json = response.json()

    assert response_json["response"] == DEFAULT_NOT_FOUND_RESPONSE
    assert response_json["request"]["model_name"] == request_data["model_name"]
    assert response_json["model_name"] == expected_model_in_response

    assert "response_id" in response_json
    assert isinstance(UUID(response_json["response_id"]), UUID)

    assert "request" in response_json
    assert response_json["request"]["prompt"] == request_data["prompt"]
    assert response_json["request"]["session_id"] == request_data["session_id"]

    assert "created_at" in response_json
    assert isinstance(datetime.fromisoformat(response_json["created_at"]), datetime)

    assert "completed_at" in response_json
    assert isinstance(datetime.fromisoformat(response_json["completed_at"]), datetime)

    assert "elapsed_time_ms" in response_json
    assert isinstance(response_json["elapsed_time_ms"], float)

@pytest.mark.asyncio
async def test_chat_stream_endpoint_not_found(client: AsyncClient):
    """Test streaming endpoint with a prompt not in mock data."""
    request_data = {
        "prompt": "Also does not exist", 
        "session_id": "api_test_stream_nf",
        "model_name": "mock-stream-request-nf"
    }
    response = await client.post("/api/v1/chat/stream", json=request_data)
    assert response.status_code == 200
    streamed_text = await response.aread()
    assert streamed_text.decode() == DEFAULT_NOT_FOUND_RESPONSE

# Error handling tests can be added similarly, potentially by creating
# mock functions that raise exceptions and overriding dependencies with those. 